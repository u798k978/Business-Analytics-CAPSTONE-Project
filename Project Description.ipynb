{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac19f28",
   "metadata": {},
   "source": [
    "# CA9.1: Creating Predictions from Text\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "The goal of this assignment is to build and evaluate a model that predicts the numerical star rating (e.g., 1 to 5 stars) of an Amazon product review based *only* on the text content of the review (title and body).\n",
    "\n",
    "**Dataset:**\n",
    "\n",
    "You will use the processed Amazon electronics review dataset created in Module 9.1.\n",
    "* **File:** `processed_electronics_reviews_openrouter_threaded.jsonl`\n",
    "* **Source:** This file contains the original review data along with the LLM-extracted insights (sentiment, categories, etc.). For this assignment, you primarily need the `rating`, `title`, and `text` fields.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "This is a **regression task**. You need to predict the numerical value in the `rating` column using the information in the `title` and `text` columns.\n",
    "\n",
    "**Your Choices (Choose at least one approach):**\n",
    "\n",
    "You can choose one or both of the following approaches, similar to those explored in Module 9.3 (which focused on classification), but adapted for **regression**:\n",
    "\n",
    "1.  **Approach 1: Fine-Tuning a Pre-trained Language Model (PLM) for Regression:**\n",
    "    * Take a pre-trained transformer model (like `distilbert-base-uncased` used in M9.3).\n",
    "    * Fine-tune it directly to predict the numerical rating based on the review text.\n",
    "    * This involves modifying the model's final layer(s) for regression output and training it on the review text and corresponding ratings.\n",
    "\n",
    "2.  **Approach 2: Feature Extraction (Embeddings) + Traditional Regression Model:**\n",
    "    * Use a pre-trained model (like `distilbert-base-uncased` or a `SentenceTransformer` model like `all-MiniLM-L6-v2` from M9.2) to generate fixed numerical embeddings (feature vectors) for each review text.\n",
    "    * Train a standard machine learning *regression* model (e.g., Linear Regression, Ridge, SVR, Gradient Boosting Regressor, etc.) using these embeddings as input features to predict the rating.\n",
    "\n",
    "**Steps & Instructions:**\n",
    "\n",
    "1.  **Setup:** Import necessary libraries (pandas, numpy, torch, transformers, datasets, evaluate, scikit-learn, etc.).\n",
    "2.  **Load Data:** Load the `processed_electronics_reviews_openrouter_threaded.jsonl` file into a pandas DataFrame.\n",
    "3.  **Prepare Data:**\n",
    "    * Clean the data: Filter out any rows that had processing errors in Module 9.1 (check `error` or `llm_analysis_error` fields). Ensure the `text` and `rating` columns are present and valid (not null/empty). You might combine `title` and `text` for a richer input.\n",
    "    * You may choose to focus on reviews for a specific product (ASIN) first, as done in M9.2, to manage dataset size, or use a sample of the full dataset.\n",
    "    * Split your data into training and testing sets (or use the existing train/test splits if you loaded the original dataset and processed both parts).\n",
    "4.  **Implement Your Chosen Approach(es):**\n",
    "    * Follow the specific hints below for either fine-tuning or feature extraction.\n",
    "5.  **Train:** Train your model(s) on the training data.\n",
    "6.  **Evaluate:** Evaluate your model(s) on the test data using appropriate regression metrics.\n",
    "7.  **Report:** Summarize your findings.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "**Setup and Data Loading:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3312ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm # Or standard tqdm\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "PROCESSED_FILE_PATH = r\"processed_electronics_reviews_openrouter_threaded.jsonl\"\n",
    "# Optional: Set a limit for faster testing, None to use all\n",
    "MAX_REVIEWS_TO_LOAD = 10000 # Example: Load first 10k\n",
    "\n",
    "# --- Load Data ---\n",
    "print(f\"Loading data from {PROCESSED_FILE_PATH}...\")\n",
    "if not os.path.exists(PROCESSED_FILE_PATH):\n",
    "    raise FileNotFoundError(f\"Input file not found: {PROCESSED_FILE_PATH}\")\n",
    "\n",
    "all_data = []\n",
    "with open(PROCESSED_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(tqdm(f, desc=\"Reading JSONL\")):\n",
    "        if MAX_REVIEWS_TO_LOAD is not None and i >= MAX_REVIEWS_TO_LOAD:\n",
    "            print(f\"\\nReached limit of {MAX_REVIEWS_TO_LOAD} reviews.\")\n",
    "            break\n",
    "        try:\n",
    "            all_data.append(json.loads(line))\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Skipping invalid JSON on line {i+1}\")\n",
    "\n",
    "if not all_data:\n",
    "    raise ValueError(\"No valid data loaded.\")\n",
    "\n",
    "df = pd.json_normalize(all_data, sep='_')\n",
    "print(f\"Loaded {len(df)} total entries into DataFrame.\")\n",
    "\n",
    "# --- Data Preparation & Cleaning ---\n",
    "print(\"Preparing text and filtering valid reviews...\")\n",
    "\n",
    "# Combine title and text\n",
    "df['input_text'] = df['title'].fillna('') + ' - ' + df['text'].fillna('')\n",
    "\n",
    "# Define potential error columns based on M9.1 output\n",
    "error_cols = [col for col in ['error', 'llm_analysis_error'] if col in df.columns]\n",
    "\n",
    "# Basic filtering: Keep rows with valid text, valid rating, and no errors recorded\n",
    "filter_condition = df['input_text'].str.strip().astype(bool)\n",
    "filter_condition &= df['rating'].notna() # Ensure rating is present\n",
    "\n",
    "# Check for errors - keep rows where *all* existing error columns are NA\n",
    "for col in error_cols:\n",
    "    filter_condition &= df[col].isna()\n",
    "\n",
    "df_valid = df[filter_condition].copy()\n",
    "\n",
    "# Ensure rating is numeric (e.g., float)\n",
    "df_valid['rating'] = pd.to_numeric(df_valid['rating'], errors='coerce')\n",
    "df_valid.dropna(subset=['rating'], inplace=True) # Drop rows where rating couldn't be converted\n",
    "\n",
    "print(f\"Filtered down to {len(df_valid)} valid reviews for modeling.\")\n",
    "\n",
    "if df_valid.empty:\n",
    "    raise ValueError(\"No valid reviews found after filtering. Cannot proceed.\")\n",
    "\n",
    "# --- Optional: Select a subset for faster iteration ---\n",
    "# df_sample = df_valid.sample(n=5000, random_state=42) # Example: Use 5000 reviews\n",
    "# X = df_sample['input_text'].tolist()\n",
    "# y = df_sample['rating'].values\n",
    "\n",
    "# --- Use the full valid dataset ---\n",
    "X = df_valid['input_text'].tolist() # Features (text)\n",
    "y = df_valid['rating'].values # Target (rating)\n",
    "\n",
    "# --- Split Data (Example using sklearn) ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train_text)}\")\n",
    "print(f\"Test set size: {len(X_test_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45def03e",
   "metadata": {},
   "source": [
    "**Approach 1: Fine-Tuning Hints (Refer to M9.3 Section 3)**\n",
    "\n",
    "* **Model:** Use `AutoModelForSequenceClassification` but configure it for regression by setting `num_labels=1`.\n",
    "\n",
    "    ```python\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "    import torch\n",
    "\n",
    "    model_checkpoint = \"distilbert-base-uncased\" # Or another suitable model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    # Load model for REGRESSION (num_labels=1)\n",
    "    model_finetune = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=1 # Key change for regression!\n",
    "    ).to(device)\n",
    "    ```\n",
    "\n",
    "* **Tokenization:** The tokenization process is the same as in M9.3. You'll need to tokenize `X_train_text` and `X_test_text`. Create a Dataset object compatible with the `Trainer`.\n",
    "\n",
    "    ```python\n",
    "    # (Tokenization function similar to M9.3)\n",
    "    def tokenize_function(texts):\n",
    "         return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "    # Create datasets (example structure)\n",
    "    class RegressionDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, texts, labels, tokenizer):\n",
    "            self.encodings = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "            self.labels = labels\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float) # Use float for regression\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "    train_dataset = RegressionDataset(X_train_text, y_train, tokenizer)\n",
    "    test_dataset = RegressionDataset(X_test_text, y_test, tokenizer)\n",
    "    ```\n",
    "\n",
    "* **Metrics:** Adapt the `compute_metrics` function for regression. Use metrics like Mean Absolute Error (MAE) or Mean Squared Error (MSE). `evaluate` library has these (`evaluate.load('mae')`, `evaluate.load('mse')`). Lower values are better.\n",
    "\n",
    "    ```python\n",
    "    import evaluate\n",
    "    import numpy as np\n",
    "\n",
    "    mae_metric = evaluate.load(\"mae\")\n",
    "    mse_metric = evaluate.load(\"mse\")\n",
    "\n",
    "    def compute_metrics_regression(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        # Logits are the direct output of the regression model (shape: batch_size, 1)\n",
    "        predictions = logits.squeeze(-1) # Remove the last dimension\n",
    "        mae = mae_metric.compute(predictions=predictions, references=labels)\n",
    "        mse = mse_metric.compute(predictions=predictions, references=labels)\n",
    "        return {\n",
    "            \"mae\": mae['mae'],\n",
    "            \"mse\": mse['mse'],\n",
    "            \"rmse\": np.sqrt(mse['mse']) # Calculate RMSE from MSE\n",
    "        }\n",
    "    ```\n",
    "\n",
    "* **Training Arguments:** Set `evaluation_strategy`, `save_strategy`, `logging_strategy`. Crucially, set `metric_for_best_model` to your primary evaluation metric (e.g., `\"mae\"` or `\"rmse\"`) and ensure `greater_is_better=False`.\n",
    "\n",
    "    ```python\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"review_rating_regressor_finetuned\",\n",
    "        num_train_epochs=3, # Adjust as needed\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16, # Adjust based on GPU memory\n",
    "        per_device_eval_batch_size=16,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"mae\", # Or \"rmse\"\n",
    "        greater_is_better=False, # Lower error is better\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\",\n",
    "        save_total_limit=1\n",
    "    )\n",
    "    ```\n",
    "\n",
    "* **Trainer:** Instantiate the `Trainer` with the model, args, datasets, tokenizer, and the `compute_metrics_regression` function.\n",
    "* **Train & Evaluate:** Run `trainer.train()` and `trainer.evaluate()`.\n",
    "\n",
    "**Approach 2: Feature Extraction Hints (Refer to M9.3 Section 4 / M9.2)**\n",
    "\n",
    "* **Embedding Model:** You can use `AutoModel` (like M9.3) to get hidden states (e.g., `[CLS]` token) or `SentenceTransformer` (like M9.2) for sentence embeddings. `SentenceTransformer` is often simpler for this purpose.\n",
    "\n",
    "    ```python\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    # Option 1: Sentence Transformer (often easier)\n",
    "    embedding_model_name = 'all-MiniLM-L6-v2' # Or another SentenceTransformer model\n",
    "    embedder = SentenceTransformer(embedding_model_name, device=device)\n",
    "\n",
    "    print(\"Generating embeddings for train set...\")\n",
    "    X_train_embeddings = embedder.encode(X_train_text, show_progress_bar=True, batch_size=64)\n",
    "    print(\"Generating embeddings for test set...\")\n",
    "    X_test_embeddings = embedder.encode(X_test_text, show_progress_bar=True, batch_size=64)\n",
    "\n",
    "    # Option 2: Using AutoModel (like M9.3 Section 4b/4c)\n",
    "    # (Requires loading AutoModel, AutoTokenizer, defining extract_hidden_states function,\n",
    "    # and mapping it over the text lists - potentially more complex setup)\n",
    "    # X_train_embeddings = ... # Result of mapping extract_hidden_states\n",
    "    # X_test_embeddings = ...\n",
    "    ```\n",
    "\n",
    "* **Prepare Data:** You now have `X_train_embeddings` and `X_test_embeddings` (NumPy arrays) and your target arrays `y_train` and `y_test`.\n",
    "* **Train Regression Model:** Use scikit-learn. A pipeline with `StandardScaler` is recommended. Choose a *regression* model.\n",
    "\n",
    "    ```python\n",
    "    from sklearn.linear_model import Ridge # Example: Ridge Regression\n",
    "    from sklearn.ensemble import GradientBoostingRegressor # Example: Gradient Boosting\n",
    "    from sklearn.svm import SVR # Example: Support Vector Regressor\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Example using Ridge Regression\n",
    "    regressor_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        # ('regressor', Ridge(alpha=1.0, random_state=42))\n",
    "        # ('regressor', SVR(C=1.0, epsilon=0.1))\n",
    "         ('regressor', GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)) # Example Gradient Boosting\n",
    "    ])\n",
    "\n",
    "    print(\"Training the regressor...\")\n",
    "    regressor_pipeline.fit(X_train_embeddings, y_train)\n",
    "    print(\"Training complete.\")\n",
    "    ```\n",
    "\n",
    "* **Evaluate:** Use scikit-learn metrics for regression.\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "    print(\"Evaluating the regressor...\")\n",
    "    y_pred = regressor_pipeline.predict(X_test_embeddings)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred) # R-squared\n",
    "\n",
    "    print(f\"\\nFeature Extraction + Regressor Results:\")\n",
    "    print(f\"  Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"  Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"  R-squared (R2): {r2:.4f}\")\n",
    "    ```\n",
    "\n",
    "**Evaluation:**\n",
    "\n",
    "* The primary metrics for this regression task are **Mean Absolute Error (MAE)** and **Root Mean Squared Error (RMSE)**.\n",
    "    * MAE tells you, on average, how many \"stars\" off your prediction was.\n",
    "    * RMSE penalizes larger errors more heavily.\n",
    "* You can also report **R-squared ($R^2$)** to understand the proportion of variance explained by your model.\n",
    "* Compare the performance of your chosen approach(es).\n",
    "\n",
    "**Submission:**\n",
    "\n",
    "* Submit your Python code, preferably as a Jupyter Notebook (`.ipynb`) file.\n",
    "* Include the output of your code cells, showing the training process (if applicable) and final evaluation metrics.\n",
    "* Add a brief markdown section in your notebook summarizing:\n",
    "    * Which approach(es) you implemented.\n",
    "    * The final MAE, RMSE, and R2 scores on the test set for each approach.\n",
    "    * Any challenges you faced or interesting observations.\n",
    "    * (Optional) If you tried both, a brief comparison of their performance and potential reasons for differences.\n",
    "\n",
    "**Optional Challenges (Extra Credit):**\n",
    "\n",
    "* Implement and compare both the fine-tuning and feature-extraction approaches.\n",
    "* Experiment with different pre-trained models (e.g., `RoBERTa`, other `SentenceTransformer` models).\n",
    "* For the feature extraction approach, try different traditional regression models (e.g., compare Linear Regression vs. Gradient Boosting vs. SVR).\n",
    "* Perform basic hyperparameter tuning for either the fine-tuning process (e.g., learning rate, epochs) or the traditional ML model (e.g., regularization strength for Ridge, `n_estimators` for Gradient Boosting).\n",
    "* Analyze the errors: Look at examples where your model's prediction was significantly wrong. Are there patterns?\n",
    "\n",
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper-embeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
